<h1><strong>The Checklist</strong></h1>
<p>Here is my list of what to look out for as I read a paper:</p>
<p><strong>Theory</strong></p>
<ul>
<li>Is the theory internally consistent?</li>
<li>Is it consistent with past literature and findings?</li>
<li>Is it novel or surprising?</li>
<li>Are elements that are excluded or simplified plausibly unimportant for the outcomes?</li>
<li>Is the theory general or specific? Are there more general theories on which this theory could draw or contribute?</li>
</ul>
<p>&nbsp;</p>
<p><strong>From Theory to Hypotheses</strong></p>
<ul>
<li>Is the theory really needed to generate the hypotheses?</li>
<li>Does the theory generate more hypotheses than considered?</li>
<li>Are the hypotheses really implied by the theory? Or are there ambiguities arising from say non-monotonicities or multiple equilibria?</li>
<li>Does the theory specify mechanisms?</li>
<li>Does the theory suggest heterogeneous effects?</li>
</ul>
<p>&nbsp;</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Are the hypotheses complex? (eg in fact 2 or 3 hypotheses bundled together)</li>
<li>Are the hypotheses falsifiable?</li>
</ul>
<p>&nbsp;</p>
<p><strong>Evidence I: Design</strong></p>
<ul>
<li>External validity: is the population examined representative of the larger population of interest?</li>
<li>External validity: Are the conditions under which they are examined consistent with the conditions of interest?</li>
<li>Measure validity: Do the measures capture the objects specified by the theory?</li>
<li>Consistency: Is the empirical model used consistent with the theory?</li>
<li>Mechanisms: Are mechanisms tested? How are they identified?</li>
<li>Replicability: Has the study been done in a way that it can be replicated?</li>
<li>Interpretation: Do the results admit rival interpretations?</li>
</ul>
<p>&nbsp;</p>
<p><strong>Evidence II: Analysis and Testing</strong></p>
<ul>
<li>Identification: are there concerns with reverse causality?</li>
<li>Identification: are there concerns of omitted variable bias?</li>
<li>Identification: does the model control for pre treatment variables only? Does it control or does it match?</li>
<li>Identification: Are poorly identified claims flagged as such?</li>
<li>Robustness: Are results robust to changes in the model, to subsetting the data, to changing the period of measurement or of analysis, to the addition or exclusion of plausible controls?</li>
<li>Standard errors: does the calculation of test statistics make use of the design? Do standard errors take account of plausibly clustering structures/differences in levels?</li>
<li>Presentation: Are the results presented in an intelligible way? Eg using fitted values or graphs? How can this be improved?</li>
<li>Interpretation: Can no evidence of effect be interpreted as evidence of only weak effects?</li>
</ul>
<p>&nbsp;</p>
<p><strong>Evidence III: Other sources of bias</strong></p>
<ul>
<li>Fishing: were hypotheses generated prior to testing? Was any training data separated from test data?</li>
<li>Measurement error: is error from sampling, case selection, or missing data plausibly correlated with outcomes?</li>
<li>Spillovers / Contamination: Is it plausible that outcomes in control units were altered because of the treatment received by the treated?</li>
<li>Compliance: Did the treated really get treatment? Did the controls really not?</li>
<li>Hawthorne effects: Are subjects modifying behavior simply because they know they are under study?</li>
<li>Measurement: Is treatment the only systematic difference between treatment and control or are there differences in how items were measured?</li>
<li>Implications of Bias: Are any sources of bias likely to work for or against the hypothesis tested?</li>
</ul>
<p>&nbsp;</p>
<p><strong>Explanation</strong></p>
<ul>
<li>Does the evidence support the particular causal account given?</li>
<li>Are mechanisms examined? Can they be?</li>
<li>Are there observable implications we might expect to see associated with different possible mechanisms?</li>
</ul>
<p><strong>Policy Implications</strong></p>
<ul>
<li>Do the policy implications really follow from the results?</li>
<li>If implemented would the policy changes have effects other thank those specified by the research?</li>
<li>Have the policy claims been tested directly?</li>
<li>Is the author overselling or underselling the findings?</li>
</ul>

<p><strong>BITSS--Day 1, Miguel</strong></p>
<ul>
<li>overview</li>
<li>principles </li>
<li></li>
<li></li>
</ul>


<p><strong>BITSS--Day 1, Misconduct (Fanelli)</strong></p>
<ul>
<li>Misconduct widespread</li>
<li>What is misconduct: FFP--fabrication (making up the data), falsification (making up/change the data with a intenetion), plagiarism   </li>
<li>Two domensions: philosophy (specific vs. broad) and intentions</li>
<li> In Sweden: Guidelines from the SRC (add)</li>
<li> Fraud: 1-5 percent (lower bound)</li>
<li> more retractions, but more possibility to retract</li>
<li> more published papers, but also more co-authors</li>
</ul>

<p><strong>BITSS--Day 1, Nelson</strong></p>
<ul>
<li>Power: what is the smallest effect size you care about, do power analysis accordingly (power analysis in prospect)</li>
<li> </li>
</ul>



<p><strong>BITSS--Day 2, Data</strong></p>
<ul>
<li> confidentiality vs anonymity (data that can not be reversed/identified once the unique are deleted) </li>
<li> actually, most data can be still identified once name/birth is deleted </li>
<li> name swapping for qualitative records (it preserfes the relative frequencies of the rest of the data)</li>
<li> you can anonymize , but we loose data quality</li>
<li> evaluate (R package: sdcMicro)</li>
</ul>

<p><strong>BITSS--Day 2, Replication</strong></p>
<ul>
<li> Definition: reproduction (use the code), robustness (different method), and replication (different lab) </li>
<li>  </li>
<li>  </li>
<li>  </li>
<li>  </li>
</ul>